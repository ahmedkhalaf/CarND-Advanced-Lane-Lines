{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "1. Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "2. Apply a distortion correction to raw images.\n",
    "3. Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "4. Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "5. Detect lane pixels and fit to find the lane boundary.\n",
    "6. Determine the curvature of the lane and vehicle position with respect to center.\n",
    "7. Warp the detected lane boundaries back onto the original image.\n",
    "8. Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Includes/Libraries\n",
    "\n",
    "Several standard python libraries are used, most notably CV2, PIL and moviepy.\n",
    "\n",
    "Code from the lessons/quizzes was also modified and reused in corresponding processing stages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import(ant) stuff\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw\n",
    "from moviepy.editor import VideoFileClip\n",
    "from moviepy.editor import *\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "#on a PC\n",
    "#%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Camera Calibration\n",
    "\n",
    "Camera calibration is preformed using several 9x6 chessboard calibration photos as follows:\n",
    "1. Coordinates of the chessboard are gathered from every photo provided under directory `camera_cal` (if chessboard corners were successfully detected).\n",
    "2. Camera calibration and distortion coefficients are calculated using `cv2.calibrateCamera()` function.\n",
    "3. Calculated paramters are stored in a file using `pickle.dump` for future retrival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((6*9,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "# Make a list of calibration images\n",
    "images = glob.glob('camera_cal/calibration*.jpg')\n",
    "\n",
    "# Step through the list and search for chessboard corners\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (9,6),None)\n",
    "    \n",
    "    # If found, add object points, image points\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "#Calculate calibration parameters\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, (1280, 720), None, None)\n",
    "\n",
    "#Store result\n",
    "with open('cv2.calibrateCamera.pkl', 'wb') as f:\n",
    "    pickle.dump([mtx, dist, rvecs, tvecs], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distortion correction is performed on test images using the `cv2.undistort()` function.\n",
    "The resulting images are stored under `output_images` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cv2.calibrateCamera.pkl', 'rb') as f:\n",
    "    mtx, dist, rvecs, tvecs = pickle.load(f) #Read calibration params\n",
    "\n",
    "images = glob.glob('camera_cal/calibration*.jpg') # Make a list of images\n",
    "# Step through the list to apply distortion correction and store the result\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    ret, corners = cv2.findChessboardCorners(cv2.cvtColor(dst,cv2.COLOR_BGR2GRAY), (9,6),None)\n",
    "    dst = cv2.drawChessboardCorners(dst, (9,6), corners, ret)\n",
    "    cv2.imwrite(fname.replace(\"camera_cal\",\"output_images/dist_correction\"),dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Input         | Distortion-corrected|\n",
    "|:-------------:|:-------------|\n",
    "| ![Before Calibration](camera_cal/calibration20.jpg \"Before Calibration\") | <img src=\"output_images/dist_correction/calibration20.jpg\"> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Distortion correction\n",
    "Now, Applying the same distortion correction procedure to raw images provided under directory `test_images`\n",
    "Then, storing the results under `output_images/dist_correction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob('test_images/*.jpg') # Make a list of images\n",
    "# Step through the list to apply distortion correction and store the result\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    cv2.imwrite(fname.replace(\"test_images\",\"output_images/dist_correction\"),dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Input         | Distortion-corrected|\n",
    "|:-------------:|:-------------|\n",
    "| ![Before Calibration](test_images/test1.jpg \"Raw image\") | <img src=\"output_images/dist_correction/test1.jpg\"> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating a thresholded binary image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Threshold Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return 1 corresponding to pixels within `threshold [min, max]` and 0 otherwise\n",
    "def bin_threshold_channel(channel,threshold):\n",
    "    binary = np.zeros_like(channel)\n",
    "    binary[(channel >= threshold[0]) & (channel <= threshold[1])] = 1\n",
    "    return binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For intermediate processing steps and visualization, the following utility functions are introduced to aid selective removal of certain pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the same values as input, except corresponding to pixels outside `threshold [min, max]` range are set to 0\n",
    "def threshold_channel(channel,threshold):\n",
    "    th_channel = np.copy(channel)\n",
    "    th_channel[(channel <= threshold[0]) | (channel >= threshold[1])] = 0\n",
    "    return th_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the same values as input, 0 corresponding to pixels from another `mask` outside `mask_thresh` range\n",
    "def mask_channel(channel,mask,mask_thresh):\n",
    "    masked = np.copy(channel)\n",
    "    masked[(mask <= mask_thresh[0]) | (mask >= mask_thresh[1])] = 0\n",
    "    return masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Gradient calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility funtction `gradient_calc` expects single color `channel` and applies `cv2.Sobel` in both X and Y directions.\n",
    "outputs are:\n",
    "\n",
    "1. Gradient in Y direction (scaled to 0:255)\n",
    "2. Gradient in X direction (scaled to 0:255)\n",
    "3. Gradient direction (in radiants)\n",
    "4. Absolute Gradient magnitude (scaled to 0:255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient calculations\n",
    "def gradient_calc(channel,sobel_kernel=15):\n",
    "    #Calculate the x and y gradients\n",
    "    sobelx = cv2.Sobel(channel, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(channel, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    \n",
    "    #Absolute sobel in x and y direction\n",
    "    scaled_abssobelx = np.uint8(255*sobelx/np.max(np.absolute(sobelx)))\n",
    "    scaled_abssobely = np.uint8(255*sobely/np.max(np.absolute(sobely)))\n",
    "    \n",
    "    #Gradient Direction: Take the absolute value of the gradient direction, \n",
    "    #apply a threshold, and create a binary image result\n",
    "    grad_absddir = np.arctan2(np.absolute(sobely), np.absolute(sobelx))\n",
    "    \n",
    "    #Calculate the gradient magnitude\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    #Rescale to 8 bit\n",
    "    scale_factor = np.max(gradmag)/255 \n",
    "    grad_mag = (gradmag/scale_factor).astype(np.uint8)\n",
    "    \n",
    "    return scaled_abssobely, scaled_abssobelx, grad_absddir, grad_mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Color channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While RGB color channels can be easily obtained directy as follows:\n",
    "\n",
    "`r,g,b = cv2.split(undist)`\n",
    "    \n",
    "The image needs to be converted to HLS color or LAB channels before splitting as shown below\n",
    "\n",
    "`h,l,s = cv2.split(cv2.cvtColor(undist, cv2.COLOR_RGB2HLS))`\n",
    "\n",
    "`lab_l,lab_a,lab_b = cv2.split(cv2.cvtColor(undist, cv2.COLOR_RGB2LAB))`\n",
    "\n",
    "Histogram equalization is needed in some cases\n",
    "\n",
    "`h_histeq = cv2.equalizeHist(h)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Example pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "undist = cv2.imread(\"output_images/dist_correction/test1.jpg\")\n",
    "r,g,b = cv2.split(undist)\n",
    "h,l,s = cv2.split(cv2.cvtColor(undist, cv2.COLOR_RGB2HLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gradient threshold for color channel r\n",
    "scaled_abssobely, scaled_abssobelx, grad_absddir, grad_mag = gradient_calc(r,sobel_kernel=15)\n",
    "\n",
    "#keep only pixels with strong gradient in horizontal direction\n",
    "grad_mag_masked = mask_channel(grad_mag,grad_absddir,(0.7, 1.3))\n",
    "\n",
    "#keep top 30% of gradient magnitude values\n",
    "r_min_thresh = np.max(grad_mag_masked)/3\n",
    "r_binary = bin_threshold_channel(grad_mag_masked,(r_min_thresh,r_min_thresh*3))\n",
    "\n",
    "# color threshold color channel s\n",
    "s_binary = bin_threshold_channel(s,(150,255))\n",
    "\n",
    "# combine both thresholded images\n",
    "combined_th = np.dstack((np.zeros_like(s_binary), s_binary * 255, r_binary * 255))\n",
    "\n",
    "#Show output of all steps/each step\n",
    "grad_th_steps = cv2.cvtColor( np.hstack( (grad_mag, grad_mag_masked,r_binary*255) ), cv2.COLOR_GRAY2RGB)\n",
    "color_th_steps = cv2.cvtColor( np.hstack( (s, s_binary*255) ), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "diagnostic_frame = np.vstack( (grad_th_steps,np.hstack((color_th_steps,combined_th)) ))\n",
    "\n",
    "cv2.imwrite(\"output_images/diagnostic/test1_bin_combined.jpg\",combined_th)\n",
    "cv2.imwrite(\"output_images/diagnostic/test1.jpg\",diagnostic_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Processing pipeline](output_images/diagnostic/test1.jpg \"Processing pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prespective Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate birds-eye view of lanes, the transform is calculated to produce vertical parallel lines based on expected locations of start and end point of left and right lanes\n",
    "\n",
    "Source point | Destination point\n",
    "---|---\n",
    "Left lane top|320, 0\n",
    "Left lane bottom|320, 720\n",
    "Right lane top|960, 0\n",
    "Right lane top|960, 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_src_dest_coordinates(img_size):\n",
    "    #define source points around expected left lane and right lane locations\n",
    "    src = np.float32(\n",
    "            [[(img_size[0] / 2) - 60, img_size[1] / 2 + 100],\n",
    "            [ (img_size[0] / 6)     , img_size[1]],\n",
    "            [ (img_size[0]*5/6) + 40, img_size[1]],\n",
    "            [ (img_size[0] / 2) + 65, img_size[1] / 2 + 100]])\n",
    "\n",
    "    #corresponding destination points\n",
    "    dst = np.float32(\n",
    "            [[(img_size[0]* 1 / 4), 0],\n",
    "            [(img_size[0] * 1 / 4), img_size[1]],\n",
    "            [(img_size[0] * 3 / 4), img_size[1]],\n",
    "            [(img_size[0] * 3 / 4), 0]])\n",
    "    return src, dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate transform `M` and inverse transform `Minv` from above `src` and `dest` coordinates.\n",
    "\n",
    "Afterwards, Images can be transformed using the function `cv2.warpPerspective`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(img_size):\n",
    "    src,dst = get_src_dest_coordinates(img_size)\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst,src)\n",
    "    # Return the resulting matrix\n",
    "    return M, Minv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transform is verifyed using provided test images named \"straight_lines1.jpg\" and \"straight_lines2.jpg\" (after distortion correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read image with straigt lines\n",
    "undist = cv2.imread(\"output_images/dist_correction/straight_lines1.jpg\")\n",
    "ref_img_size = (undist.shape[1], undist.shape[0])\n",
    "\n",
    "# calc transform based source and destination coordinates depending on image size\n",
    "trns_M, trns_Minv = get_transform(ref_img_size)\n",
    "\n",
    "# for visualtzation only\n",
    "src_pts, dst_pts = get_src_dest_coordinates(ref_img_size)\n",
    "\n",
    "# Warp the image using OpenCV warpPerspective()\n",
    "wraped = cv2.warpPerspective(undist, trns_M, ref_img_size)\n",
    "unwraped = cv2.warpPerspective(wraped, trns_Minv, ref_img_size)\n",
    "\n",
    "# draw reference polygon overlay\n",
    "cv2.polylines(undist, np.int32([src_pts]), 1, (0,0,255), thickness=3)\n",
    "cv2.polylines(wraped, np.int32([dst_pts]), 1, (0,0,255), thickness=3)\n",
    "cv2.polylines(unwraped, np.int32([src_pts]), 1, (0,0,255), thickness=3)\n",
    "\n",
    "# store output for reference/analysis\n",
    "cv2.imwrite(\"output_images/diagnostic/straight_lines1_marked.jpg\",undist)\n",
    "cv2.imwrite(\"output_images/diagnostic/straight_lines1_wrapped.jpg\",wraped)\n",
    "cv2.imwrite(\"output_images/diagnostic/straight_lines1_unwrapped.jpg\",unwraped)\n",
    "\n",
    "#repeat for second reference image\n",
    "undist = cv2.imread(\"output_images/dist_correction/straight_lines2.jpg\")\n",
    "# Warp the image using OpenCV warpPerspective()\n",
    "wraped = cv2.warpPerspective(undist, trns_M, ref_img_size)\n",
    "unwraped = cv2.warpPerspective(wraped, trns_Minv, ref_img_size)\n",
    "\n",
    "# draw reference polygon overlay\n",
    "cv2.polylines(undist, np.int32([src_pts]), 1, (0,0,255), thickness=3)\n",
    "cv2.polylines(wraped, np.int32([dst_pts]), 1, (0,0,255), thickness=3)\n",
    "cv2.polylines(unwraped, np.int32([src_pts]), 1, (0,0,255), thickness=3)\n",
    "\n",
    "# store output for reference/analysis\n",
    "cv2.imwrite(\"output_images/diagnostic/straight_lines2_marked.jpg\",undist)\n",
    "cv2.imwrite(\"output_images/diagnostic/straight_lines2_wrapped.jpg\",wraped)\n",
    "cv2.imwrite(\"output_images/diagnostic/straight_lines2_unwrapped.jpg\",unwraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Source Points         | Transform | Inverse transform |\n",
    "|-------------|-------------|-------------|\n",
    "| ![Undistorted](output_images/diagnostic/straight_lines1_marked.jpg \"Undistorted\") | ![Wrapped](output_images/diagnostic/straight_lines1_wrapped.jpg \"Wrapped\")  | ![Unwrapped](output_images/diagnostic/straight_lines1_unwrapped.jpg \"Unwrapped\") \n",
    "| ![Undistorted](output_images/diagnostic/straight_lines2_marked.jpg \"Undistorted\") | ![Wrapped](output_images/diagnostic/straight_lines2_wrapped.jpg \"Wrapped\")  | ![Unwrapped](output_images/diagnostic/straight_lines2_unwrapped.jpg \"Unwrapped\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Lane pixels identificiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Lane identification using sliding window Method\n",
    "Starting from binary threshold image, which contains filtered birds-eye image of the lanes, starting points are identified by searching the bottom third of the image for histogram peaks. As shown in sliding window technique quizzes.\n",
    "\n",
    "From there, we attempt to identify two columns of `nwindows` windows. These fixed-width windows are re-centered depending on activated pixels around the previous one's location.\n",
    "\n",
    "Finally, pixels falling within re-centered windows are considered pixels for left and right lane.\n",
    "Their coordinates are stored in `leftx,lefty` and `rightx,righty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lane_pixels(binary_warped):\n",
    "    # Take a histogram of the bottom third of the image\n",
    "    histogram = np.sum(binary_warped[(binary_warped.shape[0]//2):,:], axis=0)\n",
    "    # Create an output image to draw on and visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    first_third = midpoint//3\n",
    "    \n",
    "    leftx_base = np.argmax(histogram[first_third:midpoint]) + first_third\n",
    "    rightx_base = np.argmax(histogram[midpoint:-first_third]) + midpoint\n",
    "\n",
    "    # HYPERPARAMETERS\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 4\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 120\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 25\n",
    "\n",
    "    # Set height of windows - based on nwindows above and image shape\n",
    "    window_height = np.int(binary_warped.shape[0]//nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated later for each window in nwindows\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        \n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img,(win_xleft_low,win_y_low),\n",
    "        (win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "        cv2.rectangle(out_img,(win_xright_low,win_y_low),\n",
    "        (win_xright_high,win_y_high),(0,255,0), 2) \n",
    "        \n",
    "        # Identify the nonzero pixels in x and y within the window #\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        \n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices (previously was a list of lists of pixels)\n",
    "    try:\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "    except ValueError:\n",
    "        # Avoids an error if the above is not implemented fully\n",
    "        pass\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    \n",
    "    return leftx, lefty, rightx, righty, out_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Polynomial fitting\n",
    "Identified pixels are fit in second order polynomials\n",
    "![equation](http://www.sciweavers.org/tex2img.php?eq=f%28y%29%3DAy%5E2%2BBy%2BC&bc=White&fc=Black&im=jpg&fs=12&ff=arev)\n",
    "\n",
    "`left_fit` and `right_fit` hold the values `[A,B,C]` for each polynomial, representing the lane boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_poly(leftx, lefty, rightx, righty):\n",
    "    try:\n",
    "        ### TO-DO: Fit a second order polynomial to each with np.polyfit() ###\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "    except TypeError:\n",
    "        # Avoids an error if `left` and `right_fit` are still none or incorrect\n",
    "        left_fit = np.array([ 0, 1, 1])\n",
    "        right_fit = np.array([ 0, 1, 1])\n",
    "        print('Fitting Failed!')\n",
    "    return left_fit, right_fit\n",
    "\n",
    "#Calculate function for values of a variable using second order polynomial coefficients\n",
    "def calc_forV(variable,poly):\n",
    "    x = poly[0]*variable**2 + poly[1]*variable + poly[2]\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_colorize(image,x,y,color=[255, 0, 0]):\n",
    "    image[y,x]=color\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_img_size = (s_binary.shape[1], s_binary.shape[0])\n",
    "\n",
    "# Warp the image using OpenCV warpPerspective()\n",
    "s_wraped = cv2.warpPerspective(s_binary, trns_M, ref_img_size)\n",
    "r_wraped = cv2.warpPerspective(r_binary, trns_M, ref_img_size)\n",
    "bin_wraped = (s_wraped + r_wraped)* 255\n",
    "\n",
    "color_binary = np.dstack((np.zeros_like(bin_wraped), s_wraped*255, r_wraped*255)) \n",
    "\n",
    "cv2.imwrite(\"output_images/diagnostic/test1_bin_wraped.jpg\", color_binary)\n",
    "\n",
    "leftx, lefty, rightx, righty, out_img = find_lane_pixels(bin_wraped)\n",
    "\n",
    "# Colors in the left and right lane regions\n",
    "color_binary[lefty, leftx] = [255, 0, 0]\n",
    "color_binary[righty, rightx] = [255, 0, 0]\n",
    "out_img[lefty, leftx] = [255, 0, 0]\n",
    "out_img[righty, rightx] = [0, 0, 255]\n",
    "\n",
    "cv2.imwrite(\"output_images/diagnostic/test1_bin_lane_pixels.jpg\", out_img)\n",
    "\n",
    "left_fit, right_fit = fit_poly(leftx, lefty, rightx, righty)\n",
    "\n",
    "# Generate x and y values for plotting\n",
    "ploty = np.linspace(0, color_binary.shape[0]-1, color_binary.shape[0])\n",
    "### TO-DO: Calc both polynomials using ploty, left_fit and right_fit ###\n",
    "left_fitx = calc_forV(ploty, left_fit) #left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "right_fitx = calc_forV(ploty, right_fit) #right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "#left_fitx, right_fitx, ploty = fit_poly(wraped.shape, leftx, lefty, rightx, righty)\n",
    "\n",
    "color_binary[np.int32(ploty), np.int32(left_fitx)] = [255, 255, 255]\n",
    "color_binary[np.int32(ploty), np.int32(right_fitx)] = [255, 255, 255]\n",
    "\n",
    "cv2.imwrite(\"output_images/diagnostic/test1_bin_lane_fit.jpg\", color_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Possible lane pixels | lane identification |\n",
    "|-------------|-------------|\n",
    "| ![Wraped bin filter](output_images/diagnostic/test1_bin_wraped.jpg \"Wraped bin filter\") | ![Sliding Windows](output_images/diagnostic/test1_bin_lane_pixels.jpg \"Sliding Windows\")  |\n",
    "\n",
    "##### Polynomial fitting result\n",
    "\n",
    "![Poly Fit](output_images/diagnostic/test1_bin_lane_fit.jpg \"Poly Fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Radius of curvature and vehicle position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to to determine the curvature of the lanes at the image bottom.\n",
    "Using the equations descibed in \"Measuring Curvature\" lesson, it can be calculated based on the polynomial fitting step outcome.\n",
    "\n",
    "Vehicle position with respect to center is calculated using lane center position at the image bottom as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y-value where we want radius of curvature\n",
    "# We'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "y_eval = np.max(ref_img_size[1])\n",
    "\n",
    "# Calculation of R_curve (radius of curvature)\n",
    "left_curverad = ((1 + (2*left_fit[0]* y_eval + left_fit[1]) **2)**1.5) / np.absolute(2*left_fit[0])\n",
    "right_curverad= ((1 + (2*right_fit[0]*y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
    "\n",
    "pixels_from_center = (ref_img_size[0]/2) - ((right_fitx[-1] + left_fitx[-1]) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the calculations for the real world using meters per pixel ratio in X and Y dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left lane radius    2851.78 pixels     935.76 meters\n",
      "Right lane radius   35745.18 pixels   11675.08 meters\n",
      "Position from center     -29.52 pixels      -0.16 meters\n"
     ]
    }
   ],
   "source": [
    "# Define conversions in x and y from pixels space to meters\n",
    "ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "#Polynomial scale\n",
    "poly_scal_coeff = [xm_per_pix / (ym_per_pix ** 2), (xm_per_pix/ym_per_pix), 1]\n",
    "left_fit_cr_scaled  =  left_fit * poly_scal_coeff\n",
    "right_fit_cr_scaled = right_fit * poly_scal_coeff\n",
    "\n",
    "meters_from_center = pixels_from_center * xm_per_pix\n",
    "\n",
    "left_curverad_scaled =  ((1 + ((2* left_fit_cr_scaled[0]* (y_eval*ym_per_pix)) +  left_fit_cr_scaled[1])**2)**1.5) / np.absolute(2 *left_fit_cr_scaled[0])\n",
    "right_curverad_scaled = ((1 + ((2*right_fit_cr_scaled[0]* (y_eval*ym_per_pix)) + right_fit_cr_scaled[1])**2)**1.5) / np.absolute(2*right_fit_cr_scaled[0])\n",
    "\n",
    "print(\"Left lane radius {:10.2f} pixels {:10.2f} meters\".format(left_curverad,left_curverad_scaled))\n",
    "print(\"Right lane radius {:10.2f} pixels {:10.2f} meters\".format(right_curverad,right_curverad_scaled))\n",
    "print(\"Position from center {:10.2f} pixels {:10.2f} meters\".format(pixels_from_center,meters_from_center))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Warp lane boundaries back onto the original image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_lane_region(result,left_fitx,right_fitx,ploty,InvTransform,color=(255,0,150)):\n",
    "    # Create an image to draw the lines on\n",
    "    color_warp = np.zeros_like(result).astype(np.uint8)\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "    int_pts = np.int_([pts])\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, int_pts, color)\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, InvTransform, (result.shape[1], result.shape[0]))\n",
    "    \n",
    "    # Add it as 60% colored overlay\n",
    "    result = cv2.addWeighted(result, 1, newwarp, 0.6, 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Overlay result visualization onto original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_calc_results(image,left_r,right_r,distance):\n",
    "    font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    bottomLeftCornerOfText = (100,50)\n",
    "    fontScale              = 1\n",
    "    fontColor              = (255,255,255)\n",
    "    lineType               = 1\n",
    "\n",
    "    cv2.putText(image,\"Radius of curvature L{:10.2f}m R{:10.2f}m\".format(left_r,right_r), \n",
    "        (100,50), \n",
    "        font, \n",
    "        fontScale,\n",
    "        fontColor,\n",
    "        lineType)\n",
    "\n",
    "    cv2.putText(image,\"Pos from center {:10.2f}m\".format(distance), \n",
    "        (100,100), \n",
    "        font, \n",
    "        fontScale,\n",
    "        fontColor,\n",
    "        lineType)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = cv2.imread(\"output_images/dist_correction/test1.jpg\")\n",
    "result = overlay_lane_region(result,left_fitx,right_fitx,ploty,trns_Minv)\n",
    "result = print_calc_results(result,left_curverad_scaled,right_curverad_scaled,meters_from_center)\n",
    "cv2.imwrite(\"output_images/diagnostic/test1_overlay_result.jpg\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Result overlay](output_images/diagnostic/test1_overlay_result.jpg \"Result overlay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Processing Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, the pipleline explained above was integrated in one function called from `moviepy` as an image filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoState():\n",
    "    frame_size = (1280, 720)\n",
    "    frameIdx = 0\n",
    "    M = Minv = mtx = dist = rvecs = tvecs = []\n",
    "    IsDiagnosticVideo = False\n",
    "\n",
    "def initialize_video_processing(VideoClip, camera_calibration_pickle = 'cv2.calibrateCamera.pkl'):  \n",
    "    #fix bug when no subclip is selected size isn't a tuple\n",
    "    VideoState.frame_size = (VideoClip.size[0],VideoClip.size[1])\n",
    "    \n",
    "    # calc transform based source and destination coordinates depending on image size\n",
    "    VideoState.M, VideoState.Minv = get_transform(VideoState.frame_size)\n",
    "    \n",
    "    with open(camera_calibration_pickle, 'rb') as f:\n",
    "        mtx, dist, rvecs, tvecs = pickle.load(f) #Read calibration params\n",
    "    \n",
    "    VideoState.mtx = mtx\n",
    "    VideoState.dist = dist\n",
    "    \n",
    "    return\n",
    "\n",
    "def run_pipline_on_video_frame(img):\n",
    "    #easy reference for parameters\n",
    "    M = VideoState.M\n",
    "    Minv = VideoState.Minv\n",
    "    mtx = VideoState.mtx\n",
    "    dist = VideoState.dist\n",
    "    img_size = VideoState.frame_size\n",
    "    \n",
    "    #Camera distortion correction using parameters calculated earlier\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    \n",
    "    #Color channel selection\n",
    "    r,g,b = cv2.split(undist) #BGR or RGB?\n",
    "    h,l,s = cv2.split(cv2.cvtColor(undist, cv2.COLOR_RGB2HLS))\n",
    "    gray = cv2.cvtColor(undist,cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    #Gradient threshold for color channel r\n",
    "    scaled_abssobely, scaled_abssobelx, grad_absddir, grad_mag = gradient_calc(r,sobel_kernel=15)\n",
    "    #keep only pixels with strong gradient in horizontal direction\n",
    "    r_grad_mag_masked = mask_channel(grad_mag,grad_absddir,(0.7, 1.3))\n",
    "    r_min_thresh = np.max(r_grad_mag_masked)/3\n",
    "    r_bin = bin_threshold_channel(r_grad_mag_masked,(r_min_thresh,r_min_thresh*3))\n",
    "    r_bin_wraped = cv2.warpPerspective(r_bin, M, img_size)\n",
    "    \n",
    "    # color threshold color channel s\n",
    "    s_bin = bin_threshold_channel(s,(150,255))\n",
    "    s_bin_wrapped = cv2.warpPerspective(s_bin, M, img_size)\n",
    "\n",
    "    # color threshold color channel h\n",
    "    h_bin = bin_threshold_channel(h,(15,30))\n",
    "    h_bin_wraped = cv2.warpPerspective(h_bin, M, img_size)\n",
    "    \n",
    "    bin_wraped = (r_bin_wraped + s_bin_wrapped + h_bin_wraped) * 255\n",
    "    #color_binary = np.dstack((np.zeros_like(bin_wraped), s_wraped*255, r_wraped*255))\n",
    "    color_binary = np.dstack((bin_wraped, bin_wraped, bin_wraped))\n",
    "    \n",
    "    ## Calculations ##\n",
    "    # Find our lane pixels first\n",
    "    leftx, lefty, rightx, righty, out_img = find_lane_pixels(bin_wraped)\n",
    "\n",
    "    # Colors in the left and right lane regions\n",
    "    color_binary[lefty, leftx] = [255, 0, 0]\n",
    "    color_binary[righty, rightx] = [255, 0, 0]\n",
    "    out_img[lefty, leftx] = [255, 0, 0]\n",
    "    out_img[righty, rightx] = [0, 0, 255]\n",
    "\n",
    "    left_fit, right_fit = fit_poly(leftx, lefty, rightx, righty)\n",
    "\n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, color_binary.shape[0]-1, color_binary.shape[0])\n",
    "    ### TO-DO: Calc both polynomials using ploty, left_fit and right_fit ###\n",
    "    left_fitx = calc_forV(ploty, left_fit) #left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = calc_forV(ploty, right_fit) #right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    \n",
    "    left_fitx[left_fitx >= img_size[0]] = img_size[0]-1 \n",
    "    right_fitx[right_fitx >= img_size[0]] = img_size[0]-1\n",
    "    left_fitx[left_fitx < 0] = 0\n",
    "    right_fitx[right_fitx < 0] = 0\n",
    "    \n",
    "    color_binary[np.int32(ploty), np.int32(left_fitx)] = [255, 255, 255]\n",
    "    color_binary[np.int32(ploty), np.int32(right_fitx)] = [255, 255, 255]\n",
    "    \n",
    "    #cv2.imwrite(\"output_images/diagnostic/video_test_color_lanes{}.jpg\".format(time.strftime(\"%Y%m%d%H%M%S\")),out_img)\n",
    "    #cv2.imwrite(\"output_images/diagnostic/video_test_color_bin{}.jpg\".format(time.strftime(\"%Y%m%d%H%M%S\")),color_binary)\n",
    "    \n",
    "    # Define y-value where we want radius of curvature\n",
    "    # We'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    y_eval = np.max(ploty)\n",
    "\n",
    "    # Calculation of R_curve (radius of curvature)\n",
    "    left_curverad = ((1 + (2*left_fit[0]* y_eval + left_fit[1]) **2)**1.5) / np.absolute(2*left_fit[0])\n",
    "    right_curverad= ((1 + (2*right_fit[0]*y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
    "\n",
    "    pixels_from_center = (img_size[0]/2) - ((right_fitx[-1] + left_fitx[-1]) / 2)\n",
    "\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "    #Polynomial scale\n",
    "    poly_scal_coeff = [xm_per_pix / (ym_per_pix ** 2), (xm_per_pix/ym_per_pix), 1]\n",
    "    left_fit_cr_scaled  =  left_fit * poly_scal_coeff\n",
    "    right_fit_cr_scaled = right_fit * poly_scal_coeff\n",
    "\n",
    "    meters_from_center = pixels_from_center * xm_per_pix\n",
    "\n",
    "    left_curverad_scaled =  ((1 + ((2* left_fit_cr_scaled[0]* (y_eval*ym_per_pix)) +  left_fit_cr_scaled[1])**2)**1.5) / np.absolute(2 *left_fit_cr_scaled[0])\n",
    "    right_curverad_scaled = ((1 + ((2*right_fit_cr_scaled[0]* (y_eval*ym_per_pix)) + right_fit_cr_scaled[1])**2)**1.5) / np.absolute(2*right_fit_cr_scaled[0])\n",
    "    \n",
    "    result = overlay_lane_region(undist,left_fitx,right_fitx,ploty,Minv)\n",
    "    result = print_calc_results(result,left_curverad_scaled,right_curverad_scaled,meters_from_center)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video video_samples/project_video_20181128205247.mp4\n",
      "[MoviePy] Writing video video_samples/project_video_20181128205247.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [10:01<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: video_samples/project_video_20181128205247.mp4 \n",
      "\n",
      "CPU times: user 7min 11s, sys: 35.1 s, total: 7min 46s\n",
      "Wall time: 10min 4s\n"
     ]
    }
   ],
   "source": [
    "project_video = 'project_video.mp4'\n",
    "\n",
    "project_video_clip = VideoFileClip(project_video)\n",
    "initialize_video_processing(project_video_clip)\n",
    "processed_clip = project_video_clip.fl_image(run_pipline_on_video_frame) #NOTE: this function expects color images!!\n",
    "%time processed_clip.write_videofile(\"video_samples/project_video_{}.mp4\".format(time.strftime(\"%Y%m%d%H%M%S\")), audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Sample output\n",
    "\n",
    "Output saved in the mp4 video below.\n",
    "\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"video_samples/project_video_20181128205247.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "As expected, [sample output video](video_samples/project_video_20181128205247.mp4 \"sample output Video\") shows several frames with incorrect or failed lane detections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Video Exploration\n",
    "\n",
    "Looking into diagnostic images for individual frames for debugging, proved impractical and time consuming.\n",
    "\n",
    "There are so many differrent combinations of approaches to deal with variance in lighting conditions and situations througout the complete video. In addition, information from previous frames are not utilized in single frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_stack(var):\n",
    "    lst=[]\n",
    "    for i,img in enumerate(var):\n",
    "        lst.append(cv2.applyColorMap(img, cv2.COLORMAP_JET))\n",
    "    return np.hstack( tuple(lst) )\n",
    "def diagnostic_colorspaces(img):\n",
    "    #easy reference for parameters\n",
    "    M = VideoState.M\n",
    "    Minv = VideoState.Minv\n",
    "    mtx = VideoState.mtx\n",
    "    dist = VideoState.dist\n",
    "    img_size = VideoState.frame_size\n",
    "    VideoState.frameIdx = VideoState.frameIdx + 1\n",
    "    \n",
    "    #Camera distortion correction using parameters calculated earlier\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    #Visual guidance for RGB layers\n",
    "    cv2.putText(undist,\"R\",(100,50),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),1)\n",
    "    cv2.putText(undist,\"  G\",(100,50),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),1)\n",
    "    cv2.putText(undist,\"    B\",(100,50),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),1)\n",
    "    \n",
    "    #Color channel selection\n",
    "    rgb_r,rgb_g,rgb_b = cv2.split(undist) #BGR or RGB?\n",
    "    hls_h,hls_l,hls_s = cv2.split(cv2.cvtColor(undist, cv2.COLOR_RGB2HLS))\n",
    "    hsv_h,hsv_s,hsv_v = cv2.split(cv2.cvtColor(undist, cv2.COLOR_RGB2HSV))\n",
    "    yuv_y,yuv_u,yuv_v = cv2.split(cv2.cvtColor(undist, cv2.COLOR_RGB2YUV))\n",
    "    \n",
    "    # combine both thresholded images\n",
    "    #combined_th = np.dstack((np.zeros_like(s_binary), s_binary * 255, r_binary * 255))\n",
    "\n",
    "    #Show output of all steps/each step\n",
    "    #grad_th_steps = cv2.cvtColor( np.hstack( (grad_mag, grad_mag_masked,r_binary*255) ), cv2.COLOR_GRAY2RGB)\n",
    "    #color_th_steps = cv2.cvtColor( np.hstack( (s, s_binary*255) ), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    diagnostic_frame = np.vstack( (h_stack((rgb_r,rgb_g,rgb_b)),\n",
    "                                   h_stack((hls_h,hls_l,hls_s)),\n",
    "                                   h_stack((hsv_h,hsv_s,hsv_v)),\n",
    "                                   h_stack((yuv_y,yuv_u,yuv_v))))\n",
    "    \n",
    "    cv2.putText(diagnostic_frame,\"Idx={}\".format(VideoState.frameIdx),(200,50),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),1)\n",
    "    \n",
    "    return cv2.resize(diagnostic_frame, (0,0), fx=0.25, fy=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video video_samples/full_diag_project_video_20181130182851.mp4\n",
      "[MoviePy] Writing video video_samples/full_diag_project_video_20181130182851.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [08:07<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: video_samples/full_diag_project_video_20181130182851.mp4 \n",
      "\n",
      "CPU times: user 4min 46s, sys: 1min 8s, total: 5min 54s\n",
      "Wall time: 8min 10s\n"
     ]
    }
   ],
   "source": [
    "project_video = 'project_video.mp4'\n",
    "harder_challenge = 'harder_challenge_video.mp4'\n",
    "\n",
    "project_video_clip = VideoFileClip(project_video)\n",
    "initialize_video_processing(project_video_clip)\n",
    "processed_clip = project_video_clip.fl_image(diagnostic_colorspaces) #NOTE: this function expects color images!!\n",
    "%time processed_clip.write_videofile(\"video_samples/full_diag_project_video_{}.mp4\".format(time.strftime(\"%Y%m%d%H%M%S\")), audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video video_samples/full_diag_harder_challenge_video_20181130183703.mp4\n",
      "[MoviePy] Writing video video_samples/full_diag_harder_challenge_video_20181130183703.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1199/1200 [08:27<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: video_samples/full_diag_harder_challenge_video_20181130183703.mp4 \n",
      "\n",
      "CPU times: user 4min 31s, sys: 1min 5s, total: 5min 36s\n",
      "Wall time: 8min 32s\n"
     ]
    }
   ],
   "source": [
    "project_video = 'harder_challenge_video.mp4'\n",
    "\n",
    "project_video_clip = VideoFileClip(project_video)\n",
    "initialize_video_processing(project_video_clip)\n",
    "processed_clip = project_video_clip.fl_image(diagnostic_colorspaces) #NOTE: this function expects color images!!\n",
    "%time processed_clip.write_videofile(\"video_samples/full_diag_harder_challenge_video_{}.mp4\".format(time.strftime(\"%Y%m%d%H%M%S\")), audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a color map helps us choose the better method for each color layer and what happens under different lighting/noise conditions.\n",
    "\n",
    "After applying [cv::COLORMAP_JET ](https://docs.opencv.org/3.4/d3/d50/group__imgproc__colormap.html#gga9a805d8262bcbe273f16be9ea2055a65ab3f207661ddf74511b002b1acda5ec09) it's easy to see when lanes are further distinct from surroundings (blue vs. red) or when they're not (light blue vs. yellow).\n",
    "The closer the colors, the more difficult it is to use color threshold methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, [diagnostic video](video_samples/full_diag_project_video_20181130182851.mp4 \"diagnostic Video\") makes it easier to choose certain color channels which rely some distinctive features of the lanes reliably.\n",
    "\n",
    "It's also clear that, some noisy situations will require gradient filtering rather than color filtering.\n",
    "\n",
    "This gives us a starting point but isn't quite sufficient.\n",
    "\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"video_samples/full_diag_project_video_20181130182851.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Augmented video diagnostics\n",
    "\n",
    "Then, we tweak the pipleine utilizing the same technique on intermediate ouput of pipeline stages.\n",
    "\n",
    "This time, we also focus on tricky section(s) of the videos and fine-tune the parameters and processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipline_on_video_frame_with_diag(img):\n",
    "    #easy reference for parameters\n",
    "    M = VideoState.M\n",
    "    Minv = VideoState.Minv\n",
    "    mtx = VideoState.mtx\n",
    "    dist = VideoState.dist\n",
    "    img_size = VideoState.frame_size\n",
    "    \n",
    "    #Camera distortion correction using parameters calculated earlier\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    \n",
    "    #Color channel selection\n",
    "    #r,g,b = cv2.split(undist) #BGR or RGB?\n",
    "    #h,l,s = cv2.split(cv2.cvtColor(undist, cv2.COLOR_RGB2HLS))\n",
    "    #gray = cv2.cvtColor(undist,cv2.COLOR_RGB2GRAY)\n",
    "    #Color channel selection\n",
    "    rgb_r,rgb_g,rgb_b = cv2.split(undist) #BGR or RGB?\n",
    "    hls_h,hls_l,hls_s = cv2.split(cv2.cvtColor(undist, cv2.COLOR_RGB2HLS))\n",
    "    hsv = cv2.cvtColor(undist, cv2.COLOR_RGB2HSV)\n",
    "    hsv_h,hsv_s,hsv_v = cv2.split(hsv)\n",
    "    yuv_y,yuv_u,yuv_v = cv2.split(cv2.cvtColor(undist, cv2.COLOR_RGB2YUV))\n",
    "    \n",
    "    #Gradient threshold for color channel r\n",
    "    scaled_abssobely, scaled_abssobelx, grad_absddir, grad_mag = gradient_calc(rgb_r,sobel_kernel=15)\n",
    "    #keep only pixels with strong gradient in horizontal direction\n",
    "    r_grad_mag_masked = mask_channel(grad_mag,grad_absddir,(0.7, 1.3))\n",
    "    r_min_thresh = np.max(r_grad_mag_masked)/3\n",
    "    r_bin = bin_threshold_channel(r_grad_mag_masked,(r_min_thresh,r_min_thresh*3))\n",
    "    r_bin_wraped = cv2.warpPerspective(r_bin, M, img_size)\n",
    "    \n",
    "    # color threshold color channel s\n",
    "    s_bin = bin_threshold_channel(hls_s,(150,255))\n",
    "    s_bin_wrapped = cv2.warpPerspective(s_bin, M, img_size)\n",
    "    s_scaled_abssobely, s_scaled_abssobelx, s_grad_absddir, s_grad_mag = gradient_calc(hls_s,sobel_kernel=15)\n",
    "    s_grad_mag_masked = mask_channel(s_grad_mag,s_grad_absddir,(0.8, 1.2))\n",
    "    s_max_thresh = np.max(s_grad_mag_masked)\n",
    "    s_min_thresh = s_max_thresh*2/3\n",
    "    sg_bin = bin_threshold_channel(s_grad_mag_masked,(s_min_thresh,s_max_thresh))\n",
    "    sg_bin_wraped = cv2.warpPerspective(sg_bin, M, img_size)\n",
    "    \n",
    "    # color threshold color channel h\n",
    "    h_bin = bin_threshold_channel(hls_h,(10,20))\n",
    "    h_bin_wraped = cv2.warpPerspective(h_bin, M, img_size)\n",
    "    h_scaled_abssobely, h_scaled_abssobelx, h_grad_absddir, h_grad_mag = gradient_calc(s_bin,sobel_kernel=15)\n",
    "    h_grad_mag_masked = mask_channel(h_grad_mag,h_grad_absddir,(0.8, 1.2))\n",
    "    h_max_thresh = np.max(h_grad_mag_masked)\n",
    "    h_min_thresh = h_max_thresh*2/3\n",
    "    hg_bin = bin_threshold_channel(h_grad_mag_masked,(h_min_thresh,h_max_thresh))\n",
    "    hg_bin_wraped = cv2.warpPerspective(hg_bin, M, img_size)\n",
    "    \n",
    "    #Gradient threshold for color channel r\n",
    "    y_scaled_abssobely, y_scaled_abssobelx, y_grad_absddir, y_grad_mag = gradient_calc(yuv_y,sobel_kernel=15)\n",
    "    #keep only pixels with strong gradient in horizontal direction\n",
    "    y_grad_mag_masked = mask_channel(y_grad_mag,y_grad_absddir,(0.7, 1.3))\n",
    "    y_max_thresh = np.max(y_grad_mag_masked)\n",
    "    y_min_thresh = np.max(y_grad_mag_masked)/3\n",
    "    y_bin = bin_threshold_channel(y_grad_mag_masked,(r_min_thresh,y_max_thresh))\n",
    "    y_bin_wraped = cv2.warpPerspective(y_bin, M, img_size)\n",
    "    \n",
    "    yellow_hsv_low  = np.array([ 0,  100,  100])\n",
    "    yellow_hsv_high = np.array([ 80, 255, 255])\n",
    "    hsv_yellow = cv2.inRange(hsv, yellow_hsv_low, yellow_hsv_high)\n",
    "    \n",
    "    white_hsv_range = 30\n",
    "    white_hsv_low  = np.array([ 0,   0,   255-white_hsv_range])\n",
    "    white_hsv_high = np.array([ 255, white_hsv_range, 255])\n",
    "    \n",
    "    hsv_white = cv2.inRange(hsv, white_hsv_low, white_hsv_high)\n",
    "    \n",
    "    #hsv_color = bin_threshold_channel(cv2.cvtColor(cv2.bitwise_or(hsv_white,hsv_yellow),cv2.COLOR_RGB2GRAY),(10,255))\n",
    "    hsv_color = hsv_white + hsv_yellow\n",
    "    hsv_color_wraped = cv2.warpPerspective(hsv_color, M, img_size)\n",
    "    \n",
    "    if(VideoState.IsDiagnosticVideo):\n",
    "        diagnostic_frame = np.vstack( (h_stack((rgb_r,scaled_abssobelx,grad_mag  ,r_grad_mag_masked)),\n",
    "                                       h_stack((hls_s,s_scaled_abssobelx,s_grad_mag,s_grad_mag_masked)),\n",
    "                                       h_stack((hls_h,h_scaled_abssobelx,h_grad_mag,h_grad_mag_masked)),\n",
    "                                       h_stack((yuv_y,y_scaled_abssobelx,y_grad_mag,y_grad_mag_masked))))\n",
    "\n",
    "        binary_output_frame = (np.hstack((bin_threshold_channel(hls_h,(0,10)),\n",
    "                                          bin_threshold_channel(hls_h,(20,30)),\n",
    "                                          r_bin_wraped , hsv_color , hsv_color_wraped , sg_bin_wraped , hg_bin_wraped , y_bin_wraped)) * 255)\n",
    "\n",
    "    bin_wraped = ((r_bin_wraped + sg_bin_wraped + hg_bin_wraped + y_bin_wraped + hsv_color_wraped) * 255) #((s_bin_wrapped + h_bin_wraped)* 64) \n",
    "    #color_binary = np.dstack((np.zeros_like(bin_wraped), s_wraped*255, r_wraped*255))\n",
    "    color_binary = np.dstack((bin_wraped, bin_wraped, bin_wraped))\n",
    "    \n",
    "    ## Calculations ##\n",
    "    # Find our lane pixels first\n",
    "    leftx, lefty, rightx, righty, out_img = find_lane_pixels(bin_wraped)\n",
    "\n",
    "    # Colors in the left and right lane regions\n",
    "    color_binary[lefty, leftx] = [255, 0, 0]\n",
    "    color_binary[righty, rightx] = [255, 0, 0]\n",
    "    out_img[lefty, leftx] = [255, 0, 0]\n",
    "    out_img[righty, rightx] = [0, 0, 255]\n",
    "\n",
    "    left_fit, right_fit = fit_poly(leftx, lefty, rightx, righty)\n",
    "\n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, color_binary.shape[0]-1, color_binary.shape[0])\n",
    "    ### TO-DO: Calc both polynomials using ploty, left_fit and right_fit ###\n",
    "    left_fitx = calc_forV(ploty, left_fit) #left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = calc_forV(ploty, right_fit) #right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    \n",
    "    left_fitx[left_fitx >= img_size[0]] = img_size[0]-1 \n",
    "    right_fitx[right_fitx >= img_size[0]] = img_size[0]-1\n",
    "    left_fitx[left_fitx < 0] = 0\n",
    "    right_fitx[right_fitx < 0] = 0\n",
    "    \n",
    "    color_binary[np.int32(ploty), np.int32(left_fitx)] = [255, 255, 255]\n",
    "    color_binary[np.int32(ploty), np.int32(right_fitx)] = [255, 255, 255]\n",
    "    \n",
    "    #cv2.imwrite(\"output_images/diagnostic/video_test_color_lanes{}.jpg\".format(time.strftime(\"%Y%m%d%H%M%S\")),out_img)\n",
    "    #cv2.imwrite(\"output_images/diagnostic/video_test_color_bin{}.jpg\".format(time.strftime(\"%Y%m%d%H%M%S\")),color_binary)\n",
    "    \n",
    "    # Define y-value where we want radius of curvature\n",
    "    # We'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    y_eval = np.max(ploty)\n",
    "\n",
    "    # Calculation of R_curve (radius of curvature)\n",
    "    left_curverad = ((1 + (2*left_fit[0]* y_eval + left_fit[1]) **2)**1.5) / np.absolute(2*left_fit[0])\n",
    "    right_curverad= ((1 + (2*right_fit[0]*y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
    "\n",
    "    pixels_from_center = (img_size[0]/2) - ((right_fitx[-1] + left_fitx[-1]) / 2)\n",
    "\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "    #Polynomial scale\n",
    "    poly_scal_coeff = [xm_per_pix / (ym_per_pix ** 2), (xm_per_pix/ym_per_pix), 1]\n",
    "    left_fit_cr_scaled  =  left_fit * poly_scal_coeff\n",
    "    right_fit_cr_scaled = right_fit * poly_scal_coeff\n",
    "\n",
    "    meters_from_center = pixels_from_center * xm_per_pix\n",
    "\n",
    "    left_curverad_scaled =  ((1 + ((2* left_fit_cr_scaled[0]* (y_eval*ym_per_pix)) +  left_fit_cr_scaled[1])**2)**1.5) / np.absolute(2 *left_fit_cr_scaled[0])\n",
    "    right_curverad_scaled = ((1 + ((2*right_fit_cr_scaled[0]* (y_eval*ym_per_pix)) + right_fit_cr_scaled[1])**2)**1.5) / np.absolute(2*right_fit_cr_scaled[0])\n",
    "    \n",
    "    result = overlay_lane_region(undist,left_fitx,right_fitx,ploty,Minv)\n",
    "    result = print_calc_results(result,left_curverad_scaled,right_curverad_scaled,meters_from_center)\n",
    "    \n",
    "    if(VideoState.IsDiagnosticVideo):\n",
    "        result = np.vstack((np.hstack((result,cv2.resize(diagnostic_frame, (0,0), fx=0.25, fy=0.25))),\n",
    "                             cv2.cvtColor(cv2.resize(binary_output_frame, (0,0), fx=1/4, fy=1/4),cv2.COLOR_GRAY2RGB),\n",
    "                          np.hstack((out_img,color_binary))))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video video_samples/augmented_project_video_s35_20181204073743.mp4\n",
      "[MoviePy] Writing video video_samples/augmented_project_video_s35_20181204073743.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 250/251 [06:18<00:01,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: video_samples/augmented_project_video_s35_20181204073743.mp4 \n",
      "\n",
      "CPU times: user 4min 39s, sys: 14.3 s, total: 4min 53s\n",
      "Wall time: 6min 29s\n"
     ]
    }
   ],
   "source": [
    "project_video = 'project_video.mp4'\n",
    "\n",
    "VideoState.IsDiagnosticVideo = True\n",
    "project_video_clip = VideoFileClip(project_video).subclip(35,45)\n",
    "initialize_video_processing(project_video_clip)\n",
    "processed_clip = project_video_clip.fl_image(run_pipline_on_video_frame_with_diag) #NOTE: this function expects color images!!\n",
    "%time processed_clip.write_videofile(\"video_samples/augmented_project_video_s35_{}.mp4\".format(time.strftime(\"%Y%m%d%H%M%S\")), audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmented diagnostic and final video output saved in the mp4 video below.\n",
    "\n",
    "After several changes in the pipeline [augmented diangnostic video](video_samples/augmented_project_video_s35_20181204073743.mp4 \"sample augmented diagnostic Video\") shows how the pipeline stages behave under different conditions.\n",
    "\n",
    "\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"video_samples/augmented_project_video_s35_20181204073743.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "To summerize changes in the pipeline, after visual analysis of the intermediate results of the different filters, it became clear that:\n",
    "\n",
    "1. Color threshold on single channel isn't reliable in many situations. a [good alternative using hsv for tracking yellow was described here](https://towardsdatascience.com/robust-lane-finding-using-advanced-computer-vision-techniques-mid-project-update-540387e95ed3) some enhancements were required as suggested by the resources on [Object Tracking\n",
    "](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.html#object-tracking) and [Tracking white using opencv](https://stackoverflow.com/questions/22588146/tracking-white-color-using-python-opencv)\n",
    "2. Gradient directional threshold for `hls_s` and `hls_h` need to be more selective\n",
    "3. window margin for `find_lane_pixels` step needs to be wider in order to obtain better curve-following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video video_samples/full_augmented_project_video20181203211246.mp4\n",
      "[MoviePy] Writing video video_samples/full_augmented_project_video20181203211246.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1261 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/1261 [00:01<25:57,  1.24s/it]\u001b[A\n",
      "100%|█████████▉| 1260/1261 [31:58<00:01,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: video_samples/full_augmented_project_video20181203211246.mp4 \n",
      "\n",
      "CPU times: user 23min 49s, sys: 56 s, total: 24min 45s\n",
      "Wall time: 32min 9s\n"
     ]
    }
   ],
   "source": [
    "project_video = 'project_video.mp4'\n",
    "\n",
    "VideoState.IsDiagnosticVideo = True\n",
    "project_video_clip = VideoFileClip(project_video)\n",
    "initialize_video_processing(project_video_clip)\n",
    "processed_clip = project_video_clip.fl_image(run_pipline_on_video_frame_with_diag) #NOTE: this function expects color images!!\n",
    "%time processed_clip.write_videofile(\"video_samples/full_augmented_project_video{}.mp4\".format(time.strftime(\"%Y%m%d%H%M%S\")), audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final video output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video video_output/full_project_video20181204074413.mp4\n",
      "[MoviePy] Writing video video_output/full_project_video20181204074413.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [22:02<00:01,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: video_output/full_project_video20181204074413.mp4 \n",
      "\n",
      "CPU times: user 19min 38s, sys: 3.4 s, total: 19min 41s\n",
      "Wall time: 22min 5s\n"
     ]
    }
   ],
   "source": [
    "project_video = 'project_video.mp4'\n",
    "\n",
    "VideoState.IsDiagnosticVideo = False\n",
    "project_video_clip = VideoFileClip(project_video)\n",
    "initialize_video_processing(project_video_clip)\n",
    "processed_clip = project_video_clip.fl_image(run_pipline_on_video_frame_with_diag) #NOTE: this function expects color images!!\n",
    "%time processed_clip.write_videofile(\"video_output/full_project_video{}.mp4\".format(time.strftime(\"%Y%m%d%H%M%S\")), audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Final output video](video_output/full_project_video20181204074413.mp4 \"output Video\") is saved in the mp4 video below.\n",
    "\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"video_output/full_project_video20181204074413.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Discussion\n",
    "\n",
    "#### 11.1 Implementation Notes\n",
    "\n",
    "1. The current pipelines fails to follow sudden changes in lane radius, treating the whole image as one curve doesn't seem to work all the time even under normal conditions\n",
    "2. Camera is basically blind in several segments of harder challenge\n",
    "3. When there is only one lane line visible, radius of curvature is relatively small, or the vehicle isn't driving near the right direction the current implementation will fail\n",
    "\n",
    "#### 11.2 Possible enhancements\n",
    "\n",
    "1. Split each photo into slightly overlapping segments, fit each segment individually\n",
    "2. Several performance enhancements possible for example: searching for lane pixels around prior fit\n",
    "3. Smoothing (low-pass filtering) lane positions\n",
    "4. Applying DeepLearning techniques to enhance lane detection\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
